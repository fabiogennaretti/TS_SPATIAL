<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Régression robuste aux valeurs extrêmes</title>

<script src="libs/header-attrs-2.18/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div id="header">



<h1 class="title toc-ignore">Régression robuste aux valeurs
extrêmes</h1>

</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Les modèles linéaires classiques – test <span
class="math inline">\(t\)</span>, ANOVA et régression linéaire – sont
basés sur la comparaison des moyennes entre différents groupes ou
différentes valeurs d’un prédicteur, avec une incertitude basée sur le
calcul de la variance résiduelle. Les coefficients d’un modèle linéaire
sont estimés par la méthode des moindres carrés, qui vise à minimiser
cette variance résiduelle.</p>
<p>Ces méthodes sont conçues pour être optimales lorsque la variation
résiduelle suit une distribution normale, qui prévoit relativement peu
de valeurs extrêmes. La présence de quelques valeurs extrêmes exerce une
forte influence sur les estimés produits par ces méthodes et rend
difficile la détection des effets représentés par la plus grande partie
des données. Dans ce cours, nous verrons plusieurs alternatives à la
régression linéaire classique qui sont plus résistantes, ou
<em>robustes</em>, à la présence de valeurs extrêmes.</p>
<div id="contenu-du-cours" class="section level2">
<h2>Contenu du cours</h2>
<ul>
<li><p>Sensibilité aux valeurs extrêmes</p></li>
<li><p>Régression robuste avec les M-estimateurs</p></li>
<li><p>Régression <span class="math inline">\(t\)</span></p></li>
<li><p>Régression quantile</p></li>
</ul>
</div>
</div>
<div id="sensibilité-aux-valeurs-extrêmes" class="section level1">
<h1>Sensibilité aux valeurs extrêmes</h1>
<div id="mesures-de-tendance-centrale" class="section level2">
<h2>Mesures de tendance centrale</h2>
<p>Une mesure de tendance centrale vise à identifier le centre d’une
distribution; la moyenne et la médiane en sont deux exemples bien
connus. Le centre défini par la moyenne équilibre la <em>somme des
écarts</em> de part et d’autre de la valeur moyenne, tandis que celui
défini par la médiane équilibre le <em>nombre d’observations</em> de
part et d’autre. Pour cette raison, l’ajout d’une valeur extrême à un
échantillon peut affecter fortement sa moyenne, mais très peu sa
médiane.</p>
<p>Par exemple, prenons les 10 valeurs suivantes, dont la moyenne (44)
et la médiane (43.5) sont approximativement égales:</p>
<p><code>18 29 30 40 43 44 48 49 56 83</code></p>
<p>Si on ajoutait la valeur 580 à cet échantillon, la nouvelle médiane
serait de 44, tandis que la moyenne serait d’environ 93 et ne
représenterait plus une valeur “typique” de l’échantillon.</p>
</div>
<div id="point-de-rupture" class="section level2">
<h2>Point de rupture</h2>
<p>Le point de rupture (<em>breakdown point</em>) d’un estimateur est
défini par la question suivante: combien de valeurs extrêmes, si elles
sont assez extrêmes, peuvent affecter sans limite la valeur de l’estimé?
On l’exprime généralement comme une fraction du nombre
d’observations.</p>
<p>Avec <span class="math inline">\(n\)</span> observations, la moyenne
a un point de rupture de <span class="math inline">\(1/n\)</span>, car
une seule observation extrême suffit à l’entraîner vers des valeurs
extrêmes. Dans l’exemple précédent, si on augmentait la valeur extrême
ajoutée, la moyenne pourrait augmenter sans limite.</p>
<p>Dans le cas de la médiane, elle réagirait de la même façon à toute
valeur extrême ajoutée d’un côté de la distribution, peu importe la
magnitude de cette valeur extrême (la nouvelle médiane serait de 44 peu
importe si la donnée ajoutée était de 100 ou 300 ou 1000). Pour faire
augmenter la médiane sans limite, c’est toute la moitié supérieure du
jeu de données qu’il faudrait faire augmenter; la médiane a donc un
point de rupture de 0.5.</p>
</div>
<div id="précision-des-estimés-et-valeurs-extrêmes"
class="section level2">
<h2>Précision des estimés et valeurs extrêmes</h2>
<p>Nous avons vu que la valeur de la moyenne est sensible à l’ajout de
valeurs extrêmes d’un côté de la distribution (cas asymétrique). Si les
valeurs extrêmes apparaissent de façon symétrique de part et d’autre de
la moyenne, sa valeur reste inchangée. Cependant, puisque l’écart-type
de la distribution est aussi sensible aux valeurs extrêmes, la précision
avec laquelle on peut estimer de la moyenne est affectée.</p>
<p>Dans le graphique ci-dessous, la courbe verte représente une
distribution normale centrée réduite, <span class="math inline">\(y \sim
N(0, 1)\)</span>. La courbe orange représente le mélange de deux
distributions: 95% des observations proviennent de la distribution <span
class="math inline">\(N(0, 1)\)</span> et 5% proviennent d’une
distribution avec un écart-type plus grand: <span
class="math inline">\(N(0, 5)\)</span>. Ce mélange représente le cas où
la plupart des observations suivent une distribution normale, sauf une
petite fraction dont les valeurs sont plus extrêmes qu’attendu. Sur une
échelle linéaire de la densité de probabilité <span
class="math inline">\(f(y)\)</span> (à gauche), les deux distributions
apparaissent très semblables. Sur une échelle logarithmique (à droite),
on voit clairement que les valeurs extrêmes sont beaucoup plus probables
pour la distribution de mélange (ex.: environ 30 fois plus probable
d’obtenir <span class="math inline">\(y = -4\)</span> ou <span
class="math inline">\(y = 4\)</span>).</p>
<pre><code>## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.</code></pre>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Comparons maintenant les erreurs-types pour la moyenne et la médiane
de ces distributions. Pour ce faire, nous simulons 1000 échantillons de
100 observations de chacune des deux distributions; dans le cas de la
distribution de mélange, l’écart-type est de 1 pour les 95 premières
observations et de 5 pour les 5 dernières.</p>
<pre class="r"><code>set.seed(82)
norm_samp &lt;- replicate(1000, rnorm(100)) # par défaut, mean = 0, sd = 1
mix_samp &lt;- replicate(1000, rnorm(100, mean = 0, sd = c(rep(1, 95), rep(5, 5))))</code></pre>
<p>Pour la distribution normale, l’erreur-type de la moyenne obtenue par
simulation est d’environ 0.10, tel que prévu par la formule <span
class="math inline">\(\sigma / \sqrt{n} = 1 / \sqrt{100}\)</span>. Pour
la distribution de mélange, l’erreur-type est environ 50% plus élevée
(0.15).</p>
<pre class="r"><code>sd(apply(norm_samp, 2, mean))</code></pre>
<pre><code>## [1] 0.1012396</code></pre>
<pre class="r"><code>sd(apply(mix_samp, 2, mean))</code></pre>
<pre><code>## [1] 0.1524184</code></pre>
<p>Quant à la médiane, son erreur-type est supérieure à celle de la
moyenne pour la distribution normale, mais étant moins sensible (plus
robuste) aux valeurs extrêmes, elle est estimée plus précisément pour la
distribution de mélange.</p>
<pre class="r"><code>sd(apply(norm_samp, 2, median))</code></pre>
<pre><code>## [1] 0.122032</code></pre>
<pre class="r"><code>sd(apply(mix_samp, 2, median))</code></pre>
<pre><code>## [1] 0.1311463</code></pre>
<p>Qu’est-ce que ces résultats signifient? Supposons que nous comparons
deux groupes entre lesquels la moyenne et la médiane d’une variable
réponse diffèrent; si la distribution de la variable est symétrique,
alors la moyenne est identique à la médiane pour chaque groupe. Si la
variable suit une distribution normale, il est plus facile de détecter
une différence entre les moyennes qu’entre les médianes; un test basé
sur les moyennes, comme le test <span class="math inline">\(t\)</span>,
a une plus grande puissance. En présence de valeurs extrêmes,
l’erreur-type de la moyenne augmente et un test basé sur la différence
entre médianes pourrait être plus puissant.</p>
<p>Les M-estimateurs, que nous verrons plus loin dans un contexte de
régression, sont des mesures de tendance centrale qui font un compromis
entre l’efficacité de la moyenne pour une distribution normale et la
robustesse aux valeurs extrêmes de la médiane. Lorsque la distribution
est normale, la précision de ces estimateurs s’approche de celle de la
moyenne, mais ils ont un point de rupture plus élevé et peuvent donc
mieux conserver leur précision en présence de plusieurs valeurs
extrêmes.</p>
</div>
<div id="valeurs-extrêmes-et-régression" class="section level2">
<h2>Valeurs extrêmes et régression</h2>
<p>Dans une régression linéaire simple, la moyenne de la réponse <span
class="math inline">\(y\)</span> correspond à une fonction linéaire du
prédicteur <span class="math inline">\(x\)</span>, tandis que la
variation aléatoire autour de cette moyenne est représentée par un
résidu <span class="math inline">\(\epsilon\)</span> qui suit une
distribution normale.</p>
<p><span class="math display">\[y = \beta_0 + \beta_1 x +
\epsilon\]</span></p>
<p><span class="math display">\[\epsilon \sim N(0, \sigma)\]</span></p>
<p><em>Note</em>: Les concepts présentés ici s’appliquent autant à une
régression linéaire multiple, mais le cas d’un prédicteur unique est
plus simple à illustrer.</p>
<p>Les coefficients <span class="math inline">\(\beta_0\)</span> et
<span class="math inline">\(\beta_1\)</span> sont estimés par la méthode
des moindres carrés, c’est-à-dire qu’on vise à minimiser la somme des
résidus au carré pour les <span class="math inline">\(n\)</span>
observations:</p>
<p><span class="math display">\[\sum_{i=1}^n \hat{\epsilon_i}^2 =
\sum_i^n \left( y_i - \hat{\beta_0} - \hat{\beta_1} x
\right)^2\]</span></p>
<p>Ici <span class="math inline">\(\hat{\epsilon_i}\)</span> est
l’estimé de la valeur du résidu <span class="math inline">\(i\)</span>
en fonction de la valeur estimée des coefficients.</p>
<p>Pour une régression linéaire, l’influence d’une observation sur
l’estimé des coefficients dépend de deux facteurs: la taille du résidu
de cette observation, <span
class="math inline">\(\hat{\epsilon_i}\)</span>, ainsi que le
positionnement de <span class="math inline">\(x_i\)</span>. Pour un même
<span class="math inline">\(x_i\)</span>, les résidus <span
class="math inline">\(\hat{\epsilon_i}\)</span> plus extrêmes ont une
plus grande influence; pour une même taille de résidu, ceux
correspondant à une valeur de <span class="math inline">\(x_i\)</span>
plus extrême ont aussi une plus grande influence, comme le montre le
graphique ci-dessous.</p>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Dans les deux cas, le point en orange possède le même résidu, soit
<span class="math inline">\(\epsilon = -20\)</span>. Cependant, celui
placé près de la limite supérieure de <span
class="math inline">\(x\)</span> (panneau droit) affecte davantage
l’estimé de la pente <span class="math inline">\(\hat{\beta_1}\)</span>
(ligne orange = avec ce point; ligne grise pointillée = sans ce
point).</p>
<p>Les résidus situés près des extrêmes de <span
class="math inline">\(x\)</span> exercent un grand effet de levier
(<em>leverage</em>) sur la droite de régression. Puisque la droite de
régression passe toujours par le centre de gravité du nuage de points,
<span class="math inline">\((\bar{x}, \bar{y})\)</span>, un résidu situé
plus loin du centre fait davantage “pivoter” la droite dans sa
direction.</p>
<p>La <strong>distance de Cook</strong> mesure l’influence d’un point
sur l’ajustement du modèle de régression; elle tient compte à la fois de
la magnitude de <span class="math inline">\(\hat{\epsilon_i}\)</span> et
de son effet de levier en fonction de la position en <span
class="math inline">\(x\)</span>. Généralement, une distance de Cook
supérieure à 1 indique une observation ayant une grande influence.</p>
</div>
<div id="exemple" class="section level2">
<h2>Exemple</h2>
<p>Le jeu de données <code>Animals2</code> inclus avec le package
<em>robustbase</em> contient des mesures de la masse corporelle
(<em>body</em>, en kg) et de la masse du cerveau (<em>brain</em>, en g)
pour 65 espèces animales.</p>
<pre class="r"><code>library(robustbase)
data(Animals2)
str(Animals2)</code></pre>
<pre><code>## &#39;data.frame&#39;:    65 obs. of  2 variables:
##  $ body : num  1.35 465 36.33 27.66 1.04 ...
##  $ brain: num  8.1 423 119.5 115 5.5 ...</code></pre>
<p>La relation allométrique entre ces deux grandeurs est visible sur un
graphique log-log.</p>
<pre class="r"><code>ggplot(Animals2, aes(x = body, y = brain)) +
    geom_point() +
    scale_x_log10() +
    scale_y_log10()</code></pre>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Tous les animaux dans ce jeu de données sont des mammifères, excepté
trois qui sont des dinosaures. Il s’agit des trois observations avec la
masse corporelle la plus grande, mais dont la masse du cerveau se
retrouve sous la tendance générale.</p>
<p>Dans une analyse statistique, les valeurs aberrantes
(<em>outliers</em>) peuvent être exclues si nous avons des informations
indépendantes indiquant que les mesures sont incorrectes, ou qu’elles
proviennent d’une population différente du reste des observations.
Puisqu’il est raisonnable de croire que la relation allométrique diffère
entre les mammifères et les dinosaures, il serait justifié d’exclure ces
derniers avant d’effectuer la régression. Pour les besoins du cours,
nous supposerons qu’il n’y a pas de raison <em>a priori</em> d’exclure
ces valeurs.</p>
<p>Une régression linéaire basée sur l’ensemble des données donne une
pente de 0.59 pour <em>log(brain)</em> en fonction de
<em>log(body)</em>.</p>
<pre class="r"><code>lm_ani &lt;- lm(log(brain) ~ log(body), Animals2)
summary(lm_ani)</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(brain) ~ log(body), data = Animals2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.8592 -0.5075  0.1550  0.6410  2.5724 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.17169    0.16203   13.40   &lt;2e-16 ***
## log(body)    0.59152    0.04117   14.37   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.172 on 63 degrees of freedom
## Multiple R-squared:  0.7662, Adjusted R-squared:  0.7625 
## F-statistic: 206.4 on 1 and 63 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Dans les graphiques de diagnostic d’une régression, R indique
automatiquement les numéros ou noms des rangées correspondant aux
valeurs extrêmes. Dans ce cas-ci, chaque rangée du jeu de données est
identifiée du nom de l’animal.</p>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Le 4e graphique, <em>Residuals vs. Leverage</em>, permet d’identifier
les points avec une forte influence. Les lignes pointillées démarquent
les seuils de 0.5 et 1 pour la distance de Cook. Ici, aucun des trois
points extrêmes ne dépasse 1, mais leur influence est supérieure de
beaucoup à celle du reste des points.</p>
<p>En comparaison, la régression ignorant les trois données extrêmes
donne une pente de 0.75.</p>
<pre class="r"><code>summary(lm(log(brain) ~ log(body), Animals2[-c(6,16,26),]))</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(brain) ~ log(body), data = Animals2[-c(6, 16, 
##     26), ])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.71550 -0.49228 -0.06162  0.43597  1.94829 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.13479    0.09604   22.23   &lt;2e-16 ***
## log(body)    0.75169    0.02846   26.41   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6943 on 60 degrees of freedom
## Multiple R-squared:  0.9208, Adjusted R-squared:  0.9195 
## F-statistic: 697.4 on 1 and 60 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Le résultat des deux régressions est illustré dans le graphique
suivant (courbe orange: avec dinosaures, courbe grise pointillée: sans
dinosaures, région ombragée: intervalle de confiance).</p>
<pre class="r"><code>ggplot(Animals2, aes(x = body, y = brain)) +
    geom_smooth(data = Animals2[-c(6,16,26),], method = &quot;lm&quot;, 
                alpha = 0.1, color = &quot;grey30&quot;, linetype = &quot;dashed&quot;) +
    geom_smooth(method = &quot;lm&quot;, color = &quot;#b3452c&quot;, fill = &quot;#b3452c&quot;) +
    geom_point() +
    geom_point(data = Animals2[c(6,16,26),], color = &quot;#b3452c&quot;, size = 2) +
    scale_x_log10() +
    scale_y_log10()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Dans les prochaines sections, nous verrons comment réduire
l’influence des valeurs extrêmes sans les exclure complètement de
l’analyse.</p>
</div>
</div>
<div id="régression-robuste-avec-les-m-estimateurs"
class="section level1">
<h1>Régression robuste avec les M-estimateurs</h1>
<p>Les M-estimateurs sont des mesures de la tendance centrale conçues en
fonction de deux objectifs:</p>
<ul>
<li><p>Offrir une meilleure robustesse aux valeurs extrêmes que la
moyenne: point de rupture plus élevé et une erreur-type plus faible en
présence de valeurs extrêmes.</p></li>
<li><p>Avoir une erreur-type qui s’approche de celle de la moyenne
lorsque la distribution est normale.</p></li>
</ul>
<p>Puisqu’un modèle de régression vise à estimer la moyenne d’une
variable réponse en fonction de prédicteurs, les M-estimateurs peuvent
s’appliquer à ce type de modèle.</p>
<p>Dans le contexte d’une régression, le calcul d’un M-estimateur
procède en assignant des poids à chaque résidu lors de l’application de
la méthode des moindres carrés, afin de réduire le poids des résidus
plus extrêmes.</p>
<p>Dans la méthode des moindres carrés pondérés, chaque observation a un
poids <span class="math inline">\(w_i\)</span> et on cherche à
minimiser:</p>
<p><span class="math display">\[\sum_{i=1}^n w_i^2
\hat{\epsilon_i}^2\]</span></p>
<p>Si tous les poids sont égaux à 1, on retrouve la méthode des moindres
carrés.</p>
<p>Un des premiers M-estimateurs proposés a été celui de Huber, qui
correspond aux poids <span class="math inline">\(w_i = 1\)</span> si
<span class="math inline">\(\vert \hat{\epsilon_i} \vert \le k\)</span>
et <span class="math inline">\(w_i = k/\vert \hat{\epsilon_i}
\vert\)</span> si <span class="math inline">\(\vert \hat{\epsilon_i}
\vert &gt; k\)</span>. Avec cette méthode, tous les résidus inférieurs à
<span class="math inline">\(-k\)</span> ou supérieurs à <span
class="math inline">\(k\)</span> comptent comme des résidus égaux à
<span class="math inline">\(-k\)</span> ou <span
class="math inline">\(k\)</span>, respectivement, pour le calcul des
coefficients de la régression.</p>
<p>Le bipoids de Tukey (<em>Tukey’s biweight</em>) est un autre
M-estimateur, qui correspond aux poids <span class="math inline">\(w_i =
(1 - (\hat{\epsilon_i}/k)^2)^2\)</span> si <span
class="math inline">\(\vert \hat{\epsilon_i} \vert \le k\)</span> et
<span class="math inline">\(w_i = 0\)</span> si <span
class="math inline">\(\vert \hat{\epsilon_i} \vert &gt; k\)</span>. Cet
estimateur donne donc un poids inférieur à 1 à tous les résidus; ce
poids diminue avec la magnitude du résidu pour atteindre 0 si le résidu
est inférieur à <span class="math inline">\(-k\)</span> ou supérieur à
<span class="math inline">\(k\)</span>, ce qui équivaut à exclure
complètement les résidus de cette magnitude.</p>
<p>Les valeurs de <span class="math inline">\(k\)</span> utilisées le
plus couramment sont <span class="math inline">\(k =
1.345\hat{\sigma}\)</span> pour la méthode de Huber et <span
class="math inline">\(k = 4.685\hat{\sigma}\)</span> pour le bipoids de
Tukey; les poids résultants sont illustrés dans le graphique ci-dessous.
Ici, <span class="math inline">\(\hat{\sigma}\)</span> est un estimé
robuste de l’écart-type des données, dont nous ne discuterons pas dans
ce cours. Ces valeurs de <span class="math inline">\(k\)</span> sont
choisies afin que l’erreur-type des estimés soit au plus 5% au-dessus de
celle obtenue par le modèle classique si la distribution des résidus est
normale.</p>
<p>Le graphique ci-dessous montre le poids accordé pour chacune des deux
méthodes en fonction du résidu normalisé par <span
class="math inline">\(\sigma\)</span>.</p>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Pour estimer les coefficients d’une régression robuste avec les
M-estimateurs, il faut minimiser la somme des carrés des résidus en
fonction de poids qui dépendent eux-mêmes des résidus. Afin de résoudre
ce problème, l’algorithme utilisé procède par itération (<em>iterative
reweighted least squares</em> ou IRLS):</p>
<ul>
<li><p>On commence avec une première valeur proposée pour chaque
coefficient, puis on calcule les résidus et les poids.</p></li>
<li><p>On ré-estime les coefficients en minimisant la somme des carrés
des résidus pondérés, puis on révise la valeur des résidus et des poids
selon ces nouveaux coefficients.</p></li>
<li><p>On répète l’étape précédente jusqu’à ce que les poids demeurent
stables d’une itération à l’autre selon la précision voulue.</p></li>
</ul>
<p>Parmi les deux méthodes mentionnées, le bipoids de Tukey tolère mieux
les valeurs extrêmes avec grand effet de levier. Cependant, son résultat
peut dépendre des valeurs initiales proposées par l’algorithme. La
méthode d’estimation “MM” est une variation du M-estimateur qui utilise
une autre technique robuste afin de fournir des valeurs initiales au
M-estimateur avec bipoids de Tukey. La fonction de régression linéaire
robuste <code>lmrob</code> du package <em>robustbase</em> utilise la
méthode MM comme choix par défaut.</p>
<p>Voici le résultat de <code>lmrob</code> appliquée au jeu de données
<em>Animals2</em> vu précédemment. La première partie du sommaire des
résultats ressemble au tableau obtenu avec <code>lm</code> (estimé des
coefficients, erreur-type et test de signification). Ensuite, on obtient
un sommaire des poids calculées, puis la liste des paramètres de
l’algorithme.</p>
<pre class="r"><code>lmrob_ani &lt;- lmrob(log(brain) ~ log(body), Animals2)
summary(lmrob_ani)</code></pre>
<pre><code>## 
## Call:
## lmrob(formula = log(brain) ~ log(body), data = Animals2)
##  \--&gt; method = &quot;MM&quot;
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -5.56235 -0.52597 -0.04378  0.46510  1.98894 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.11749    0.09146   23.15   &lt;2e-16 ***
## log(body)    0.74603    0.02065   36.12   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Robust residual standard error: 0.721 
## Multiple R-squared:  0.9229, Adjusted R-squared:  0.9217 
## Convergence in 8 IRWLS iterations
## 
## Robustness weights: 
##  3 observations c(6,16,26) are outliers with |weight| = 0 ( &lt; 0.0015); 
##  10 weights are ~= 1. The remaining 52 ones are summarized as
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.4269  0.8956  0.9512  0.9092  0.9829  0.9986 
## Algorithmic parameters: 
##        tuning.chi                bb        tuning.psi        refine.tol 
##         1.548e+00         5.000e-01         4.685e+00         1.000e-07 
##           rel.tol         scale.tol         solve.tol       eps.outlier 
##         1.000e-07         1.000e-10         1.000e-07         1.538e-03 
##             eps.x warn.limit.reject warn.limit.meanrw 
##         2.069e-11         5.000e-01         5.000e-01 
##      nResample         max.it       best.r.s       k.fast.s          k.max 
##            500             50              2              1            200 
##    maxit.scale      trace.lev            mts     compute.rd fast.s.large.n 
##            200              0           1000              0           2000 
##                   psi           subsampling                   cov 
##            &quot;bisquare&quot;         &quot;nonsingular&quot;         &quot;.vcov.avar1&quot; 
## compute.outlier.stats 
##                  &quot;SM&quot; 
## seed : int(0)</code></pre>
<p>La fonction <code>weights</code> permet de consulter les poids
associés à chaque observation.</p>
<pre class="r"><code>ggplot(data = NULL, aes(x = rownames(Animals2), 
                        y = weights(lmrob_ani, type = &quot;robustness&quot;))) +
    geom_point() +
    coord_flip() + # inverse la position des axes x et y
    theme_bw()</code></pre>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Notez que les trois espèces de dinosaures ont reçu un poids nul, avec
pour résultat que la droite de régression est à peu près identique à
celle obtenue avec <code>lm</code> en excluant ces trois espèces.</p>
<pre class="r"><code>ggplot(Animals2, aes(x = body, y = brain)) +
    geom_smooth(data = Animals2[-c(6,16,26),], method = &quot;lm&quot;, 
                alpha = 0.1, color = &quot;grey30&quot;, linetype = &quot;dashed&quot;) +
    geom_smooth(method = &quot;lmrob&quot;, color = &quot;#b3452c&quot;, fill = &quot;#b3452c&quot;) +
    geom_point() +
    geom_point(data = Animals2[c(6,16,26),], color = &quot;#b3452c&quot;, size = 2) +
    scale_x_log10() +
    scale_y_log10()</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<div id="extension-aux-modèles-linéaires-généralisés"
class="section level2">
<h2>Extension aux modèles linéaires généralisés</h2>
<p>Le package <em>robustbase</em> contient aussi une fonction
<code>glmrob</code>. Celles-ci applique différentes méthodes semblables
aux M-estimateurs pour produire des estimés robustes des coefficients de
modèles linéaires généralisés (GLM).</p>
</div>
</div>
<div id="régression-t" class="section level1">
<h1>Régression <span class="math inline">\(t\)</span></h1>
<p>Les méthodes présentées dans la section précédente ne sont pas basées
sur une forme spécifique de la distribution des résidus autour de la
valeur attendue de <span class="math inline">\(y\)</span>, ce qui
contribue à leur généralité.</p>
<p>Toutefois, certaines approches de modélisation, comme le maximum de
vraisemblance vu au dernier cours et les méthodes bayésiennes présentées
plus tard cette session, requièrent de spécifier une distribution pour
toutes les variables aléatoires du modèle. Dans ce cas, si on souhaite
assigner à une variable une distribution semblable à la normale, mais
qui permet davantage de valeurs extrêmes, nous pouvons avoir recours à
la distribution <span class="math inline">\(t\)</span>.</p>
<p><em>Rappel</em>: Dans les cours de statistiques, la distribution
<span class="math inline">\(t\)</span> de Student est d’abord présentée
comme une façon d’estimer la distribution de la moyenne d’un échantillon
<span class="math inline">\(\bar{x}\)</span> lorsque la variance de la
population est inconnue. Pour un échantillon de <span
class="math inline">\(n\)</span> observations, si <span
class="math inline">\(\sqrt{n}(\bar{x} - \mu)/\sigma\)</span> suit une
distribution normale centrée réduite et qu’on remplace <span
class="math inline">\(\sigma\)</span> par son estimé <span
class="math inline">\(s\)</span> à partir de l’échantillon, alors <span
class="math inline">\(\sqrt{n}(\bar{x} - \mu)/s\)</span> suit une
distribution <span class="math inline">\(t\)</span> avec <span
class="math inline">\(n-1\)</span> degrés de liberté.</p>
<p>Même à variance égale, la distribution <span
class="math inline">\(t\)</span> contient plus de valeurs extrêmes que
la distribution normale. Cet effet est plus prononcé lorsque le nombre
de degrés de liberté est faible: si <span class="math inline">\(df \le
2\)</span>, il y a tant de valeurs extrêmes que la variance ne peut pas
être définie. À l’opposé, la distribution <span
class="math inline">\(t\)</span> s’approche d’une distribution normale
si le nombre de degrés de liberté est élevé.</p>
<p>Voici par exemple le graphique des distributions <span
class="math inline">\(t\)</span> à 3 et 6 degrés de liberté, comparées à
une distribution normale centrée réduite. Sur l’échelle logarithmique,
on voit que les résidus à <span class="math inline">\(\pm 4\)</span>
sont environ 100 fois plus probables pour la distribution <span
class="math inline">\(t\)</span> à 3 degrés de liberté que pour la
distribution normale.</p>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>De façon plus générale, la distribution <span
class="math inline">\(t\)</span> peut servir à modéliser la variation
résiduelle dans tout modèle où on veut prévoir davantage de valeurs
extrêmes que prévues par la distribution normale.</p>
<p>La fonction <code>tlm</code> du package <em>hett</em> ajuste un
modèle de régression linéaire:</p>
<p><span class="math display">\[y = \beta_0 + \beta_1 x + ... +
\epsilon\]</span></p>
<p>où les résidus normalisés par un paramètre <span
class="math inline">\(\sigma\)</span> suivent une distribution <span
class="math inline">\(t\)</span> avec <span
class="math inline">\(\nu\)</span> degrés de liberté.</p>
<p><span class="math display">\[\epsilon/\sigma \sim t(\nu)\]</span></p>
<p>Notez qu’ici <span class="math inline">\(\sigma\)</span> n’est pas
égale à l’écart-type des résidus, car la variance de la distribution
<span class="math inline">\(t\)</span> est plus grande que celle d’une
distribution normale centrée réduite.</p>
<p>En appliquant ce modèle au jeu de données <em>Animals2</em>, nous
obtenons des coefficients de régression comparables (en tenant compte de
la marge d’erreur) à ceux obtenus dans la section précédente avec
<code>lmrob</code>. Ces résultats se retrouvent dans la section
<code>Location model</code> du sommaire de <code>tlm</code>.</p>
<pre class="r"><code>library(hett)
treg &lt;- tlm(log(brain) ~ log(body), data = Animals2,
            estDof = TRUE)
summary(treg)</code></pre>
<pre><code>## Location model :
## 
## Call:
## tlm(lform = log(brain) ~ log(body), data = Animals2, estDof = TRUE)
## 
## Residuals: 
##        Min          1Q      Median          3Q         Max  
## -5.415e+00  -5.039e-01  -8.369e-07   5.181e-01   2.067e+00  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.07829    0.09628   21.59   &lt;2e-16 ***
## log(body)    0.73653    0.02447   30.11   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Scale parameter(s) as estimated below)
## 
## 
## Scale Model :
## 
## Call:
## tlm(lform = log(brain) ~ log(body), data = Animals2, estDof = TRUE)
## 
## Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4484  -1.9795  -0.1566   1.2246   4.9181  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -1.2244     0.2745  -4.461 8.17e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Scale parameter taken to be  2 )
## 
## 
## Est. degrees of freedom parameter:  2.071194
## Standard error for d.o.f:  0.6678805
## No. of iterations of model : 8 in 0
## Heteroscedastic t Likelihood : -86.3654</code></pre>
<p>Il est important de spécifier <code>estDof = TRUE</code> pour estimer
le nombre de degrés de liberté, plutôt que de supposer une valeur fixe.
Cependant, cet estimé risque d’être peu précis sauf si on a beaucoup de
données. Ici, le nombre de degrés de liberté est de 2.08, avec une
erreur-type de 0.67.</p>
<p>La fonction <code>tlm</code> permet aussi à <span
class="math inline">\(\sigma\)</span> de varier en fonction des
prédicteurs, une option que nous n’utilisons pas ici. Notez que
l’<code>Intercept</code> sous <code>Scale model</code> est un estimé de
<span class="math inline">\(\log \sigma^2\)</span>.</p>
</div>
<div id="régression-quantile" class="section level1">
<h1>Régression quantile</h1>
<p>Au début du cours, nous avions présenté la médiane comme un exemple
de statistique robuste aux valeurs extrêmes. Par définition, la
probabilité qu’une variable <span class="math inline">\(y\)</span> soit
inférieure ou égale à sa médiane est de 50%; la médiane est donc un
<em>quantile</em> associé à une probabilité cumulative de 0.5. Les
quantiles autres que la médiane sont aussi des statistiques robustes,
bien que leur point de rupture soit moins élevé. Par exemple, un
quantile associé à une probabilité de 0.1 ou 0.9 a un point de rupture
correspondant à 10% de valeurs extrêmes.</p>
<p>Plutôt que de modéliser la moyenne d’une variable réponse en fonction
de prédicteurs, la régression quantile modélise un ou plusieurs
quantiles de la réponse en fonction des mêmes prédicteurs. Il peut donc
s’agir d’une méthode de régression robuste si on remplace la moyenne par
la médiane, mais la régression quantile a d’autres utilités:</p>
<ul>
<li>Modéliser une variable réponse dont la variance n’est pas homogène;
dans ce cas, la distance entre les quantiles varie en fonction de la
valeur des prédicteurs. Un exemple bien connu de régression quantile est
la courbe de croissance des enfants qui représente différents quantiles
de la distribution de taille ou de poids en fonction de l’âge.</li>
</ul>
<p><img src="../images/courbe_croissance.jpg" /></p>
<ul>
<li>Représenter un cas où un prédicteur influence les extrêmes de la
distribution davantage que son centre. Comme l’explique l’article de
Cade et Noon (2003) cité dans les références, cette dernière application
est utile dans le cas de systèmes complexes où la réponse est parfois
limitée par les facteurs mesurés, parfois par d’autres facteurs
non-mesurés. Dans ce cas, le prédicteur limite le “plafond” de la
réponse, mais exerce moins de contrôle sur son “plancher” si d’autres
facteurs sont alors limitants, tel qu’illustré dans le graphique
ci-dessous.</li>
</ul>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Nous utiliserons la fonction <code>rq</code> du package
<em>quantreg</em> pour effectuer une régression quantile.</p>
<p>Le jeu de données <code>Mammals</code> inclus avec ce package montre
la vitesse maximale connue (en km/h) de mammifères en fonction de leur
poids. Puisque l’échelle de poids varie sur plusieurs ordres de
grandeur, il est plus utile de représenter son logarithme.</p>
<pre class="r"><code>library(quantreg)
data(Mammals)
ggplot(Mammals, aes(x = log(weight), y = speed)) +
    geom_point()</code></pre>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>D’après ce graphique, il semble que le poids pourrait agir comme
facteur limitant pour la vitesse des mammifères, donc son effet devrait
être davantage ressenti sur les quantiles élevés de la distribution.</p>
<p>Pour exécuter une régression quantile <code>rq</code>, on spécifie la
formule du modèle et le jeu de données source <code>data</code> comme
dans une régression linéaire. Entre ces deux arguments, nous devons
aussi spécifier dans l’argument <code>tau</code> quels quantiles seront
modélisés. Ici, nous modéliserons les 1er et 9e déciles (0.1 et 0.9),
les 1er et 3e quartiles (0.25 et 0.75) ainsi que la médiane.</p>
<pre class="r"><code>qreg &lt;- rq(speed ~ log(weight), tau = c(0.10, 0.25, 0.5, 0.75, 0.9), 
           data = Mammals)</code></pre>
<p>Le sommaire du résultat présente les coefficients de la régression et
leur intervalle de confiance pour chaque quantile.</p>
<pre class="r"><code>summary(qreg)</code></pre>
<pre><code>## 
## Call: rq(formula = speed ~ log(weight), tau = c(0.1, 0.25, 0.5, 0.75, 
##     0.9), data = Mammals)
## 
## tau: [1] 0.1
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) 13.30752      8.74691 14.56745
## log(weight)  2.34755      1.62337  3.26536
## 
## Call: rq(formula = speed ~ log(weight), tau = c(0.1, 0.25, 0.5, 0.75, 
##     0.9), data = Mammals)
## 
## tau: [1] 0.25
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) 20.81692     18.62656 23.71090
## log(weight)  3.84176      3.32131  5.06629
## 
## Call: rq(formula = speed ~ log(weight), tau = c(0.1, 0.25, 0.5, 0.75, 
##     0.9), data = Mammals)
## 
## tau: [1] 0.5
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) 31.19403     28.66333 33.18496
## log(weight)  5.54939      4.68512  5.95244
## 
## Call: rq(formula = speed ~ log(weight), tau = c(0.1, 0.25, 0.5, 0.75, 
##     0.9), data = Mammals)
## 
## tau: [1] 0.75
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) 41.69078     38.59558 59.42984
## log(weight)  6.93824      2.56935  7.93761
## 
## Call: rq(formula = speed ~ log(weight), tau = c(0.1, 0.25, 0.5, 0.75, 
##     0.9), data = Mammals)
## 
## tau: [1] 0.9
## 
## Coefficients:
##             coefficients lower bd upper bd
## (Intercept) 55.82662     49.74724 83.80662
## log(weight)  7.10732     -3.05803 11.32294</code></pre>
<p>En appliquant la fonction <code>plot</code> à ce sommaire, nous
pouvons voir la tendance de chaque coefficient du modèle en fonction des
quantiles. À titre de comparaison, l’estimé du coefficient pour la
moyenne (modèle linéaire <code>lm</code>) est représenté par une ligne
rouge, avec un intervalle de confiance en pointillé.</p>
<pre class="r"><code>plot(summary(qreg))</code></pre>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>La fonction <code>predict</code> appliquée au résultat produit une
matrice, où chaque rangée correspond à la rangée correspondante des
données originales et chaque colonne représente la prédiction des
quantiles de la réponse (dans l’ordre) pour une rangée donnée.</p>
<pre class="r"><code>qpred &lt;- predict(qreg)
head(qpred)</code></pre>
<pre><code>##          [,1]     [,2]     [,3]      [,4]      [,5]
## [1,] 33.73009 54.23840 79.47103 102.05010 117.65681
## [2,] 32.77824 52.68070 77.22095  99.23689 114.77505
## [3,] 32.10289 51.57549 75.62449  97.24088 112.73040
## [4,] 30.31373 48.64753 71.39507  91.95297 107.31363
## [5,] 27.37280 43.83471 64.44300  83.26100  98.40985
## [6,] 27.05933 43.32171 63.70199  82.33453  97.46080</code></pre>
<p>Pour visualiser rapidement le résultat d’une régression quantile avec
un prédicteur, nous pouvons faire appel à la fonction
<code>geom_quantile</code> de <em>ggplot2</em>.</p>
<pre class="r"><code>ggplot(Mammals, aes(x = weight, y = speed)) +
    geom_point() +
    geom_quantile(quantiles = c(0.1, 0.25, 0.5, 0.75, 0.9), color = &quot;#b3452c&quot;) +
    scale_x_log10()</code></pre>
<pre><code>## Smoothing formula not specified. Using: y ~ x</code></pre>
<p><img src="04-Regression_robuste_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
</div>
<div id="résumé" class="section level1">
<h1>Résumé</h1>
<ul>
<li><p>La moyenne et la variance sont des statistiques sensibles aux
valeurs extrêmes.</p></li>
<li><p>Pour une régression linéaire, l’influence d’une observation
augmente si son résidu est grand (valeur extrême de <span
class="math inline">\(y\)</span>) ou si elle a un grand effet de levier
(valeur extrême de <span class="math inline">\(x\)</span>). La distance
de Cook mesure l’effet combiné de ces deux facteurs.</p></li>
<li><p>La régression robuste basée sur les M-estimateurs (fonction
<code>lmrob</code> du package <em>robustbase</em>) produit des estimés
presque aussi précis que la régression linéaire si les suppositions de
celle-ci sont respectées, tout en étant beaucoup moins sensibles à la
présence de quelques valeurs extrêmes.</p></li>
<li><p>La distribution <span class="math inline">\(t\)</span> offre une
méthode paramétrique pour représenter une variable comportant davantage
de valeurs extrêmes que la distribution normale. La fonction
<code>tlm</code> du package <em>hett</em> ajuste un modèle de régression
linéaire où la réponse suit une distribution <span
class="math inline">\(t\)</span> plutôt que normale autour de sa valeur
moyenne.</p></li>
<li><p>La régression quantile modélise l’effet d’un prédicteur sur
différents quantiles de la distribution de la réponse.</p></li>
</ul>
</div>
<div id="références" class="section level1">
<h1>Références</h1>
<ul>
<li><p>Cade, B.S. et Noon, B.R. (2003) A gentle introduction to quantile
regression for ecologists. <em>Frontiers in Ecology and the
Environment</em> 1: 412–420.</p></li>
<li><p>Fox, J. (2002) Robust Regression. Appendix to <em>An R and S-PLUS
Companion to Applied Regression</em>. Sage Publications, Thousands Oaks,
USA.</p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
