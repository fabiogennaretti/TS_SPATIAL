<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Maximum de vraisemblance</title>

<script src="libs/header-attrs-2.18/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>
<link href="libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div id="header">



<h1 class="title toc-ignore">Maximum de vraisemblance</h1>

</div>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Le maximum de vraisemblance est une méthode générale pour estimer les
paramètres d’un modèle statistique. Par exemple, supposons que nous
avons une série d’observations d’une variable aléatoire <span
class="math inline">\(y\)</span> et un modèle statistique potentiel pour
cette variable. Ce modèle peut inclure la dépendance de <span
class="math inline">\(y\)</span> sur d’autres variables prédictrices,
ainsi qu’une distribution statistique pour la portion non-expliquée de
la variation de <span class="math inline">\(y\)</span>. En général, un
tel modèle contient différents paramètres inconnus qui doivent être
ajustés aux données observées.</p>
<p>Selon le maximum de vraisemblance, les meilleurs estimés des
paramètres d’un modèle sont ceux qui maximisent la probabilité des
valeurs observées de la variable. Cette méthode peut être appliquée peu
importe la forme mathématique du modèle, ce qui permet de choisir les
modèles les plus compatibles avec notre compréhension des processus
naturels, sans être limités par les modèles déjà implémentés dans des
logiciels statistiques. (Les méthodes bayésiennes que nous verrons plus
tard dans le cours ont aussi cette versatilité.)</p>
<p>Si la méthode générale du maximum de vraisemblance n’a pas été
présenté dans le cours préalable à celui-ci (ECL7102), certaines des
méthodes vues dans ce cours étaient basées sur ce principe:</p>
<ul>
<li><p>La sélection de modèles au moyen de l’AIC est basée sur la
fonction de vraisemblance.</p></li>
<li><p>L’estimation des paramètres des modèles linéaires généralisés est
effectuée en maximisant la vraisemblance.</p></li>
<li><p>L’estimation des paramètres des modèles linéaires mixtes utilise
une version modifiée du maximum de vraisemblance (le maximum de
vraisemblance restreint ou REML).</p></li>
</ul>
<div id="contenu-du-cours" class="section level2">
<h2>Contenu du cours</h2>
<ul>
<li><p>Principe du maximum de vraisemblance</p></li>
<li><p>Application du maximum de vraisemblance dans R</p></li>
<li><p>Test du rapport de vraisemblance</p></li>
<li><p>Calcul des intervalles de confiance</p></li>
<li><p>Estimation de plusieurs paramètres: vraisemblance profilée et
approximation linéaire</p></li>
</ul>
</div>
</div>
<div id="principe-du-maximum-de-vraisemblance" class="section level1">
<h1>Principe du maximum de vraisemblance</h1>
<div id="fonction-de-vraisemblance" class="section level2">
<h2>Fonction de vraisemblance</h2>
<p>Supposons que nous souhaitons estimer le taux de germination d’un lot
de semences en faisant germer 20 de ces semences dans les mêmes
conditions. Si la variable <span class="math inline">\(y\)</span>
représente le nombre de semences ayant germé avec succès pour une
réalisation de l’expérience, alors <span
class="math inline">\(y\)</span> suit une distribution binomiale:</p>
<p><span class="math display">\[f(y \vert p) = {n \choose y} p^y
(1-p)^{n-y} \]</span></p>
<p>où le nombre d’essais <span class="math inline">\(n = 20\)</span>,
<span class="math inline">\(p\)</span> est la probabilité de germination
pour la population et <span class="math inline">\({n \choose y}\)</span>
représente le nombre de façons de choisir <span
class="math inline">\(y\)</span> individus parmi <span
class="math inline">\(n\)</span>. Nous écrivons <span
class="math inline">\(f(y \vert p)\)</span> pour préciser que cette
distribution de <span class="math inline">\(y\)</span> est
<em>conditionnelle</em> à une certaine valeur de <span
class="math inline">\(p\)</span>.</p>
<p>Par exemple, voici la distribution de <span
class="math inline">\(y\)</span> si <span class="math inline">\(p =
0.2\)</span>. La probabilité d’obtenir <span class="math inline">\(y =
6\)</span> dans ce cas est d’environ 0.11 (ligne pointillée sur le
graphique).</p>
<pre class="r"><code>ggplot(data.frame(x = 0:20), aes(x)) +
    labs(x = &quot;y&quot;, y = &quot;f(y|p=0.2)&quot;) +
    stat_function(fun = dbinom, n = 21, args = list(size = 20, prob = 0.2),
                  geom = &quot;bar&quot;, color = &quot;black&quot;, fill = &quot;white&quot;) +
    geom_segment(aes(x = 0, xend = 6, y = dbinom(6, 20, 0.2),
                     yend = dbinom(6, 20, 0.2)), 
                 color = &quot;#b3452c&quot;, linetype = &quot;dashed&quot;, size = 1) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0))</code></pre>
<pre><code>## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.</code></pre>
<p><img src="03-Maximum_vraisemblance_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Si nous avons observé <span class="math inline">\(y = 6\)</span>,
mais que nous ne connaissons pas <span class="math inline">\(p\)</span>,
la même équation nous permet de calculer la probabilité d’avoir obtenu
ce <span class="math inline">\(y\)</span> pour chaque valeur possible de
<span class="math inline">\(p\)</span>. Vue comme une fonction de <span
class="math inline">\(p\)</span>, plutôt que <span
class="math inline">\(y\)</span>, cette même équation correspond à la
fonction de <strong>vraisemblance</strong> (dénotée <span
class="math inline">\(L\)</span>, pour <em>likelihood</em>) de <span
class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[L(p) = f(y \vert p) = {n \choose y} p^y
(1-p)^{n-y}\]</span></p>
<p>Voici la forme de <span class="math inline">\(L(p)\)</span> pour
<span class="math inline">\(y = 6\)</span> et <span
class="math inline">\(n = 20\)</span>:</p>
<pre class="r"><code>ggplot(NULL) +
    labs(x = &quot;p&quot;, y = &quot;L(p)&quot;) +
    stat_function(fun = function(x) dbinom(6, 20, prob = x),
                  geom = &quot;density&quot;) +
    geom_segment(aes(x = 0, xend = 0.2, y = dbinom(6, 20, 0.2),
                     yend = dbinom(6, 20, 0.2)), 
                 color = &quot;#b3452c&quot;, linetype = &quot;dashed&quot;, size = 1) +
    geom_segment(aes(x = 0.2, xend = 0.2, y = 0, yend = dbinom(6, 20, 0.2)), 
                 color = &quot;#b3452c&quot;, linetype = &quot;dashed&quot;, size = 1) +
    scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))</code></pre>
<p><img src="03-Maximum_vraisemblance_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>La vraisemblance de <span class="math inline">\(p = 0.2\)</span> pour
cette observation de <span class="math inline">\(y\)</span> est donc
également de 0.11. Notons que <span class="math inline">\(f(y \vert
p)\)</span> était une distribution discrète, mais puisque <span
class="math inline">\(p\)</span> est un paramètre continu, la
vraisemblance <span class="math inline">\(L(p)\)</span> est définie pour
toutes les valeurs réelles entre 0 et 1.</p>
<p>De façon plus générale, supposons que <span class="math inline">\(y =
(y_1, y_2, ..., y_n)\)</span> est un vecteur d’observations et <span
class="math inline">\(\theta = (\theta_1, ..., \theta_m)\)</span> est un
vecteur des paramètres ajustables du modèle proposé pour expliquer ces
observations. Dans ce cas, la vraisemblance d’un vecteur spécifique de
valeurs pour <span class="math inline">\(\theta\)</span> correspond à la
probabilité conjointe des observations de <span
class="math inline">\(y\)</span>, conditionnellement à ces valeurs de
<span class="math inline">\(\theta\)</span>. Nous verrons un exemple
spécifique du calcul de <span class="math inline">\(L\)</span> pour un
modèle à plusieurs paramètres (distribution normale) dans la prochaine
section.</p>
<p><span class="math display">\[L(\theta) = p(y | \theta)\]</span></p>
<p><em>Note</em>: Même si la valeur de <span
class="math inline">\(L(\theta)\)</span> pour un <span
class="math inline">\(\theta\)</span> donné correspond à une
probabilité, la fonction de vraisemblance n’est pas une distribution de
probabilité, car dans la théorie vue ici, <span
class="math inline">\(\theta\)</span> n’est pas une variable aléatoire.
Aussi, l’intégrale d’une fonction de vraisemblance (aire sous la courbe
de <span class="math inline">\(L(\theta)\)</span> vs. <span
class="math inline">\(\theta\)</span>) n’est pas toujours égale à 1,
contrairement à celle d’une densité de probabilité.</p>
</div>
<div id="maximum-de-vraisemblance" class="section level2">
<h2>Maximum de vraisemblance</h2>
<p>Selon le principe du maximum de vraisemblance, le meilleur estimé des
paramètres du modèle selon nos observations <span
class="math inline">\(y\)</span> est le vecteur de valeurs <span
class="math inline">\(\theta\)</span> qui maximise la valeur de <span
class="math inline">\(L(\theta)\)</span>.</p>
<div id="exemple-distribution-binomiale" class="section level3">
<h3>Exemple: Distribution binomiale</h3>
<p>Pour le modèle binomial présenté plus haut, il est possible de
démontrer (voir le calcul dans le chapitre du livre de Bolker en
référence) que l’estimé de <span class="math inline">\(p\)</span> selon
le maximum de vraisemblance est donné par:</p>
<p><span class="math display">\[\hat{p} = \frac{y}{n}\]</span></p>
<p>Autrement dit, la proportion de succès dans l’échantillon est le
meilleur estimé de la probabilité de succès dans la population. Avec
<span class="math inline">\(y = 6\)</span> et <span
class="math inline">\(n = 20\)</span>, on voit que le maximum de <span
class="math inline">\(L(p)\)</span> est obtenu pour <span
class="math inline">\(p = 0.3\)</span>.</p>
<pre class="r"><code>ggplot(NULL) +
    labs(x = &quot;p&quot;, y = &quot;L(p)&quot;) +
    stat_function(fun = function(x) dbinom(6, 20, prob = x),
                  geom = &quot;density&quot;) +
    geom_segment(aes(x = 0, xend = 0.3, y = dbinom(6, 20, 0.3),
                     yend = dbinom(6, 20, 0.3)), 
                 color = &quot;#b3452c&quot;, linetype = &quot;dashed&quot;, size = 1) +
    geom_segment(aes(x = 0.3, xend = 0.3, y = 0, yend = dbinom(6, 20, 0.3)), 
                 color = &quot;#b3452c&quot;, linetype = &quot;dashed&quot;, size = 1) +
    scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))</code></pre>
<p><img src="03-Maximum_vraisemblance_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="exemple-modèle-linéaire" class="section level3">
<h3>Exemple: Modèle linéaire</h3>
<p>Dans le modèle de régression linéaire simple, la variable réponse
<span class="math inline">\(y\)</span> suit une distribution normale,
avec une moyenne dépendant linéairement du prédicteur <span
class="math inline">\(x\)</span> et un écart-type constant <span
class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[y \sim N(\beta_0 + \beta_1 x,
\sigma)\]</span></p>
<p>Ce modèle comporte trois paramètres à estimer: <span
class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span> et <span
class="math inline">\(\sigma\)</span>. La densité de probabilité d’une
observation de <span class="math inline">\(y\)</span> correspond donc
à:</p>
<p><span class="math display">\[f(y \vert \beta_0, \beta_1, \sigma) =
\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{y - \beta_0 -
\beta_1 x}{\sigma} \right)^2}\]</span></p>
<p>Si nous réalisons <span class="math inline">\(n\)</span> observations
indépendantes de <span class="math inline">\(y\)</span> (chacune avec la
valeur du prédicteur <span class="math inline">\(x\)</span>), leur
densité de probabilité conjointe est donnée par le produit (noté <span
class="math inline">\(\Pi\)</span>) des densités de probabilité
individuelles. Vue comme une fonction des paramètres, l’équation
suivante donne donc la vraisemblance conjointe de <span
class="math inline">\(\beta_0\)</span>, <span
class="math inline">\(\beta_1\)</span> et <span
class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[L(\beta_0, \beta_1, \sigma) = f(y_1,
..., y_n \vert \beta_0, \beta_1, \sigma) = \prod_{i=1}^n \frac{1}{\sigma
\sqrt{2 \pi}} e^{-\frac{1}{2} \left( \frac{y_i - \beta_0 - \beta_1
x_i}{\sigma} \right)^2}\]</span></p>
</div>
<div id="log-vraisemblance" class="section level3">
<h3>Log-vraisemblance</h3>
<p>En pratique, il est souvent plus facile de calculer la
log-vraisemblance, soit <span class="math inline">\(l = \log L\)</span>.
Puisque le logarithme est une fonction <em>monotone</em> – c’est-à-dire
que si <span class="math inline">\(L\)</span> augmente, <span
class="math inline">\(\log L\)</span> augmente aussi – alors la valeur
des paramètres qui maximise <span class="math inline">\(l\)</span>
maximisera aussi <span class="math inline">\(L\)</span>.</p>
<p>Puisqu’un logarithme transforme un produit en somme:</p>
<p><span class="math display">\[ \log(xy) = \log(x) +
\log(y)\]</span></p>
<p>la log-vraisemblance pour le problème de régression linéaire
ci-dessus correspond à:</p>
<p><span class="math display">\[l(\beta_0, \beta_1, \sigma) =
\sum_{i=1}^n \left( \log \left( \frac{1}{\sigma \sqrt{2 \pi}} \right) -
\frac{1}{2} \left( \frac{y_i - \beta_0 - \beta_1 x_i}{\sigma} \right)^2
\right)\]</span></p>
<p>où en simplifiant un peu plus:</p>
<p><span class="math display">\[l(\beta_0, \beta_1, \sigma) = n \log
\left( \frac{1}{\sigma \sqrt{2 \pi}} \right) - \frac{1}{2
\sigma^2}  \sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 x_i
\right)^2\]</span></p>
<p>Notons que les coefficient <span class="math inline">\(\beta\)</span>
apparaissent seulement dans le deuxième terme de l’équation, qui
contient la somme du carré des résidus du modèle. Plus ce terme diminue,
plus <span class="math inline">\(l\)</span> augmente, ce qui explique
pourquoi les estimés des coefficients <span
class="math inline">\(\beta\)</span> par la méthode des moindres carrés
sont les mêmes que ceux obtenus par le maximum de vraisemblance.</p>
<p>Pour des fonctions assez simples, la position du maximum peut être
déterminée en trouvant la valeur de chaque paramètre où la dérivée de
<span class="math inline">\(l\)</span> en fonction de ce paramètre est
0. En particulier, pour la variance des résidus <span
class="math inline">\(\sigma^2\)</span>, on obtient ainsi l’estimé
suivant:</p>
<p><span class="math display">\[\hat{\sigma^2} = \frac{1}{n}
\sum_{i=1}^n \left( y_i - \beta_0 - \beta_1 x_i \right)^2\]</span></p>
<p>Nous savons que cet estimateur de la variance est biaisé (pour un
estimé non-biaisé, il faudrait <span class="math inline">\(n -
1\)</span> au dénominateur). Le maximum de vraisemblance ne garantit pas
une absence de biais, mais la théorie indique que ce biais devient
négligeable pour un échantillon assez grand; dans cet exemple, la
différence entre <span class="math inline">\(n-1\)</span> et <span
class="math inline">\(n\)</span> devient moins importante quand <span
class="math inline">\(n\)</span> augmente.</p>
</div>
</div>
</div>
<div id="application-du-maximum-de-vraisemblance-dans-r"
class="section level1">
<h1>Application du maximum de vraisemblance dans R</h1>
<div id="exemple-plantes-des-îles-galapagos" class="section level2">
<h2>Exemple: Plantes des îles Galapagos</h2>
<p>Le fichier <a href="../donnees/galapagos.csv">galapagos.csv</a>
contient un jeu de données sur la richesse spécifique des plantes de 30
îles de l’archipel des Galapagos. (<em>Source</em>: Johnson, M.P. et
Raven, P.H. 1973. Species number and endemism: The Galapagos Archipelago
revisited. <em>Science</em> 179: 893–895.)</p>
<pre class="r"><code>galap &lt;- read.csv(&quot;../donnees/galapagos.csv&quot;)
str(galap)</code></pre>
<pre><code>## &#39;data.frame&#39;:    30 obs. of  8 variables:
##  $ Name     : chr  &quot;Baltra&quot; &quot;Bartolome&quot; &quot;Caldwell&quot; &quot;Champion&quot; ...
##  $ Species  : int  58 31 3 25 2 18 24 10 8 2 ...
##  $ Endemics : int  23 21 3 9 1 11 0 7 4 2 ...
##  $ Area     : num  25.09 1.24 0.21 0.1 0.05 ...
##  $ Elevation: int  346 109 114 46 77 119 93 168 71 112 ...
##  $ Nearest  : num  0.6 0.6 2.8 1.9 1.9 8 6 34.1 0.4 2.6 ...
##  $ Scruz    : num  0.6 26.3 58.7 47.4 1.9 ...
##  $ Adjacent : num  1.84 572.33 0.78 0.18 903.82 ...</code></pre>
<p>Nous modéliserons ces données avec une distribution binomiale
négative. Cette distribution est appropriée pour représenter les données
de comptage dont la variance est supérieure à celle prévue par la
distribution de Poisson.</p>
<p>Si une variable <span class="math inline">\(y\)</span> suit une
distribution de Poisson, alors sa moyenne et sa variance sont toutes
deux données par un même paramètre <span
class="math inline">\(\lambda\)</span>.</p>
<p><span class="math display">\[y \sim
\textrm{Pois}(\lambda)\]</span></p>
<p>La distribution binomiale negative comprend deux paramètres, <span
class="math inline">\(\mu\)</span> et <span
class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[y \sim \textrm{NB}(\mu,
\theta)\]</span></p>
<p>Dans ce modèle, <span class="math inline">\(y\)</span> a une moyenne
de <span class="math inline">\(\mu\)</span> et une variance de <span
class="math inline">\(\mu + \frac{\mu^2}{\theta}\)</span>. Le paramètre
<span class="math inline">\(\theta\)</span> est toujours positif. Une
petite valeur de <span class="math inline">\(\theta\)</span> représente
une distribution plus variable, tandis que si <span
class="math inline">\(\theta\)</span> est très élevé, le deuxième terme
est négligeable et la distribution tend vers celle de Poisson.</p>
<p>Comme pour la régression de Poisson, le modèle binomial négatif
utilise un lien logarithmique pour relier <span
class="math inline">\(\mu\)</span> à une fonction linéaire des
prédicteurs.</p>
<p><span class="math display">\[\log\mu = \beta_0 + \beta_1 x_1 +
\beta_2 x_2 + ...\]</span></p>
<p>Pour cet exemple, nous ajusterons le modèle du nombre d’espèces
(<em>Species</em>) en fonction de la superficie de l’île (<em>Area</em>,
en km<span class="math inline">\(^2\)</span>) et de la distance jusqu’à
l’île la plus proche (<em>Nearest</em>, en km). Nous prenons aussi le
logarithme de chaque prédicteur.</p>
</div>
<div id="utilisation-du-package-bbmle" class="section level2">
<h2>Utilisation du package <em>bbmle</em></h2>
<p>La plupart des modèles ne permettent pas de dériver analytiquement la
position du maximum de vraisemblance. Dans ce cas, nous avons recours à
des algorithmes d’optimisation qui estiment numériquement la valeur
maximale de la fonction de (log-)vraisemblance et la valeur de chaque
paramètre correspondant à ce maximum.</p>
<p>Dans R, la fonction <code>optim</code> est un outil général pour
déterminer le minimum ou maximum d’une fonction donnée. Toutefois, il
existe aussi des fonctions spécialisées au problème d’estimation par le
maximum de vraisemblance: dans ce cours, nous utiliserons la fonction
<code>mle2</code> du package <em>bbmle</em>.</p>
<p>Tout d’abord, nous devons écrire une fonction qui calcule l’opposé de
la log-vraisemblance (<em>negative log-likelihood</em>) pour notre
problème. Par convention, les algorithmes d’optimisation demandent une
fonction à minimiser, donc au lieu de maximiser la log-vraisemblance, on
minimise son opposé.</p>
<pre class="r"><code>nll_galap &lt;- function(b_0, b_area, b_near, theta) {
    mu_sp &lt;- exp(b_0 + b_area * log(galap$Area) + b_near * log(galap$Nearest))
    -sum(dnbinom(galap$Species, mu = mu_sp, size = theta, log = TRUE))
}</code></pre>
<p>La fonction <code>nll_galap</code> ci-dessus accepte quatre
paramètres qui correspondent aux trois coefficients du prédicteur
linéaire et au paramètre <span class="math inline">\(\theta\)</span> de
la distribution binomiale négative.</p>
<ul>
<li><p>La première ligne de la fonction calcule le prédicteur linéaire
et prend son exponentielle pour obtenir le nombre d’espèces moyen
<code>mu_sp</code>. <em>Rappel</em>: Dans R, la plupart des opérations
mathématiques sont effectuées en parallèle sur les vecteurs. Ainsi,
<code>mu_sp</code> contient 30 valeurs, la première calculée à partir
des valeurs des prédicteurs pour l’île 1, la deuxième pour les valeurs
de l’île 2, etc.</p></li>
<li><p>La deuxième ligne calcule la log-vraisemblance de chaque
observation selon le modèle binomial avec <code>dnbinom</code> (aussi en
parallèle), puis fait leur somme et prend l’opposé.</p></li>
</ul>
<p>Notez que nous spécifions <code>log = TRUE</code> dans
<code>dnbinom</code> pour calculer le logarithme de la vraisemblance.
Tel que vu précédemment, la log-vraisemblance d’un ensemble
d’observations est égale à la somme de leurs log-vraisemblances
individuelles tant que les observations sont indépendantes.</p>
<p>Finalement, nous chargeons le package <em>bbmle</em> et nous appelons
la fonction <code>mle2</code>. Le premier argument de cette fonction est
notre fonction calculant l’opposé de la log-vraisemblance. Nous devons
aussi spécifier pour l’argument <code>start</code> une liste des valeurs
initiales de chaque paramètre, que l’algorithme utilisera pour commencer
la recherche du maximum.</p>
<p>Le choix exact des valeurs initiales importe peu dans la plupart des
cas, mais il est recommandé de donner des valeurs plausibles (pas trop
extrêmes) des paramètres. Nous choisissons donc une valeur nulle pour
chaque coefficient, mais une valeur positive pour <span
class="math inline">\(\theta\)</span> qui doit être supérieur à
zéro.</p>
<pre class="r"><code>library(bbmle)

mle_galap &lt;- mle2(nll_galap, start = list(b_0 = 0, b_area = 0, b_near = 0, theta = 1))
mle_galap</code></pre>
<pre><code>## 
## Call:
## mle2(minuslogl = nll_galap, start = list(b_0 = 0, b_area = 0, 
##     b_near = 0, theta = 1))
## 
## Coefficients:
##        b_0     b_area     b_near      theta 
##  3.3352151  0.3544290 -0.1042696  2.7144722 
## 
## Log-likelihood: -137.98</code></pre>
<p>L’exécution de la fonction produit plusieurs avertissements
(<em>warnings</em>) dans R, qui ne sont pas montrés ici. Ceux-ci
résultent probablement de cas où l’algorithme tente d’assigner une
valeur négative à <code>theta</code> et produit une erreur. Dans ce cas
il tente simplement une nouvelle valeur.</p>
</div>
<div id="inteprétation-de-la-vraisemblance" class="section level2">
<h2>Inteprétation de la vraisemblance</h2>
<p>Remarquez que le maximum de la log-vraisemblance dans le résultat
ci-dessus est égal à -137.98, ce qui correspond à une valeur infime de
la vraisemblance:</p>
<pre class="r"><code>exp(-137.98)</code></pre>
<pre><code>## [1] 1.191372e-60</code></pre>
<p>La vraisemblance correspond à la probabilité d’obtenir exactement les
valeurs apparaissant dans le jeu de données, selon le modèle.
Considérant les nombreuses valeurs possibles pour une observation de la
variable et le fait que ces possibilités se multiplient pour chaque
observation subséquente, il n’est pas suprenant que cette probabilité
soit très faible et d’autant plus faible pour un grand échantillon.</p>
<p>La valeur absolue de la vraisemblance n’est pas vraiment
interprétable. C’est plutôt sa valeur relative qui permet de comparer
l’ajustement de plusieurs valeurs des paramètres appliqués en fonction
des mêmes données observées.</p>
<p>Néanmoins, il est difficile de travailler avec des nombres
extrêmement proches de zéro; c’est une des raisons pour lesquelles le
logarithme de la vraisemblance est utilisé en pratique.</p>
</div>
<div id="quand-utiliser-le-maximum-de-vraisemblance"
class="section level2">
<h2>Quand utiliser le maximum de vraisemblance?</h2>
<p>Pour notre exemple, nous aurions pu utiliser la fonction
<code>glm.nb</code> du package <em>MASS</em>, conçue spécialement pour
estimer les paramètres d’une régression binomiale négative. En ajustant
notre modèle avec cette fonction, nous pouvons vérifier que les
résultats concordent avec l’application de <code>mle2</code>.</p>
<pre class="r"><code>library(MASS)
glm.nb(Species ~ log(Area) + log(Nearest), galap)</code></pre>
<pre><code>## 
## Call:  glm.nb(formula = Species ~ log(Area) + log(Nearest), data = galap, 
##     init.theta = 2.714482206, link = log)
## 
## Coefficients:
##  (Intercept)     log(Area)  log(Nearest)  
##       3.3352        0.3544       -0.1043  
## 
## Degrees of Freedom: 29 Total (i.e. Null);  27 Residual
## Null Deviance:       138.7 
## Residual Deviance: 32.7  AIC: 284</code></pre>
<p>Les fonctions disponibles dans R et différents packages couvrent déjà
un bon nombre de modèles courants, incluant les modèles linéaires,
linéaires généralisés, mixtes et autres. Aussi, plusieurs modèles qui
n’apparaissent pas linéaires peuvent être linéarisés avec une
transformation appropriée. Par exemple, une loi de puissance entre le
nombre d’espèces <span class="math inline">\(S\)</span> et la
surperficie d’habitat <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[S = cA^z\]</span></p>
<p>peut être transformée en relation linéaire en prenant le logarithme
de chaque côté:</p>
<p><span class="math display">\[\log(S) = \log(c) + z
\log(A)\]</span></p>
<p>Lorsqu’une fonction spécialisée est disponible pour estimer les
paramètres d’un modèle, il est plus simple d’utiliser celle-ci plutôt
que de coder le modèle soi-même et d’appliquer le maximum de
vraisemblance.</p>
<p>Toutefois, il existe des cas où le modèle présumé pour les données ne
cadre pas dans un format standard. Voici quelques exemples en écologie
forestière.</p>
<p><strong>Ajustement d’une courbe de dispersion</strong> (ex.: Clark et
al. 1999)</p>
<p>Une façon d’estimer la capacité de dispersion d’une espèce de plantes
est d’échantillonner les graines tombant dans des pièges placés à
différentes distances de plantes mères. En particulier, on s’intéresse à
estimer la courbe de dispersion <span
class="math inline">\(f(r)\)</span> qui correspond à la probabilité
qu’une graine tombe à une distance <span
class="math inline">\(r\)</span> de son point d’origine.</p>
<p>Supposons que <span class="math inline">\(y\)</span> représente le
nombre de graines dans un des pièges et peut être représenté par une
distribution binomiale négative.</p>
<p><span class="math display">\[y_i \sim \textrm{NB}(\mu_i,
\theta)\]</span></p>
<p>Le nombre de graines moyen dans le piège <span
class="math inline">\(i\)</span>, <span
class="math inline">\(\mu_i\)</span>, correspond à la somme des
contributions de chaque plante mère <span
class="math inline">\(j\)</span> située à proximité; cette contribution
est égale au nombre de graines produites par une plante mère (<span
class="math inline">\(b\)</span>, que nous supposons fixe) multiplié par
la courbe de dispersion évaluée pour la distance <span
class="math inline">\(r_{ij}\)</span> entre le piège <span
class="math inline">\(i\)</span> et la plante <span
class="math inline">\(j\)</span>.</p>
<p><span class="math display">\[y_i \sim \textrm{NB}(\sum_j b\times
f(r_{ij}), \theta)\]</span></p>
<p>Puisque <span class="math inline">\(f\)</span> est une fonction
non-linéaire avec ses propres paramètres à ajuster, puis que la moyenne
de <span class="math inline">\(y\)</span> contient la somme de valeurs
de <span class="math inline">\(f\)</span> évaluées à différentes
distances, il est nécessaire de créer sa propre fonction de
vraisemblance et la maximiser avec un outil comme <code>mle2</code>.</p>
<p><strong>Estimation de la fonction de compétition du
voisinage</strong> (ex.: Canham et al. 2004)</p>
<p>La croissance d’arbres dans une forêt peut être réduite par la
compétition provenant de leurs voisins. Si on suppose que la compétition
exercée sur un arbre <span class="math inline">\(i\)</span> par un
voisin <span class="math inline">\(j\)</span> augmente avec le diamètre
<span class="math inline">\(D_j\)</span> de ce voisin et diminue avec la
distance <span class="math inline">\(r_{ij}\)</span> entre les deux
arbres, nous pouvons définir un indice de compétition (<em>CI</em>)
faisant la somme des effets de chaque voisin sur <span
class="math inline">\(i\)</span>:</p>
<p><span class="math display">\[CI_i = \sum_j
\frac{D_j^{\delta}}{r_{ij}^{\gamma}}\]</span></p>
<p>Nous souhaitons estimer les puissances <span
class="math inline">\(\delta\)</span> et <span
class="math inline">\(\gamma\)</span> apparaissant dans l’indice à
partir des données. Supposons que nous avons un modèle linéaire de la
croissance <span class="math inline">\(y_i\)</span> de l’arbre <span
class="math inline">\(i\)</span> auquel nous ajoutons un terme dépendant
de cet indice:</p>
<p><span class="math display">\[y_i = \beta_0 + ... + \beta_{CI} \sum_j
\frac{D_j^{\delta}}{r_{ij}^{\gamma}}\]</span></p>
<p>Il n’y a pas moyen de simplifier ce dernier terme, donc le maximum de
vraisemblance peut être utile pour estimer les coefficients (tous les
<span class="math inline">\(\beta\)</span>, <span
class="math inline">\(\gamma\)</span> et <span
class="math inline">\(\delta\)</span>) de ce modèle maintenant
non-linéaire.</p>
</div>
<div id="limites-du-maximum-de-vraisemblance" class="section level2">
<h2>Limites du maximum de vraisemblance</h2>
<p>La plupart des propriétés avantageuses des estimés du maximum de
vraisemblance, dont l’absence de biais, sont valides dans la limite où
la taille de l’échantillon est grande. Ce qui constitue un échantillon
assez grand dépend du modèle et en particulier du nombre de paramètres à
estimer.</p>
<p>En pratique, le maximum de vraisemblance est obtenu par un algorithme
numérique recherchant le maximum par un processus itératif. Une fonction
de vraisemblance complexe pourrait avoir plusieurs maximums locaux (des
points où la fonction est maximisée par rapport aux valeurs proches des
paramètres), dans lequel cas il n’est pas garanti que l’algorithme
trouve le maximum global (celui avec la vraisemblance la plus
élevée).</p>
</div>
</div>
<div id="test-du-rapport-de-vraisemblance" class="section level1">
<h1>Test du rapport de vraisemblance</h1>
<div id="test-sur-la-valeur-dun-paramètre" class="section level2">
<h2>Test sur la valeur d’un paramètre</h2>
<p>Il est possible d’utiliser la fonction de vraisemblance pour tester
une hypothèse sur la valeur d’un paramètre.</p>
<p>Par exemple, considérons la fonction de vraisemblance calculée au
début du cours pour estimer la probabilité de germination d’un lot de
semences, si 6 semences ont germé sur 20 essais.</p>
<pre class="r"><code>ggplot(NULL) +
    labs(x = &quot;p&quot;, y = &quot;L(p)&quot;) +
    stat_function(fun = function(x) dbinom(6, 20, prob = x),
                  geom = &quot;density&quot;) +
    scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))</code></pre>
<p><img src="03-Maximum_vraisemblance_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Dans ce cas, l’estimé du maximum de vraisemblance est <span
class="math inline">\(\hat{p} = 0.3\)</span>. Supposons que le
fournisseur des semences affirme que leur taux de germination est de
50%. Est-ce que le résultat de l’expérience est compatible avec cette
valeur?</p>
<p>La vraisemblance correspondant à l’hypothèse nulle (<span
class="math inline">\(p_0 = 0.5\)</span>) est d’environ <span
class="math inline">\(L(p_0) = 0.037\)</span>, comparativement à un
maximum de <span class="math inline">\(L(\hat{p}) = 0.192\)</span>.</p>
<pre class="r"><code>l_0 &lt;- dbinom(6, 20, prob = 0.5)
l_max &lt;- dbinom(6, 20, prob = 0.3)
c(l_0, l_max)</code></pre>
<pre><code>## [1] 0.03696442 0.19163898</code></pre>
<p>Le rapport entre ces deux valeurs de <span
class="math inline">\(L\)</span> sert à définir une statistique pour le
test du rapport de vraisemblance (<em>likelihood-ratio test</em>). Cette
statistique correspond à -2 fois le logarithme du rapport entre la
vraisemblance du paramètre sous l’hypothèse nulle et le maximum de
vraisemblance estimé.</p>
<p><span class="math display">\[- 2 \log \left(
\frac{L(\theta_0)}{L(\hat{\theta})} \right)\]</span></p>
<p>De façon équivalente, on peut remplacer le rapport par la différence
des log-vraisemblances:</p>
<p><span class="math display">\[- 2 \left( l(\theta_0) - l(\hat{\theta})
\right)\]</span></p>
<p>Le facteur -2 a été choisi pour que, si l’hypothèse nulle est vraie
et que l’échantillon est assez grand, la distribution de cette
statistique s’approche de la distribution du <span
class="math inline">\(\chi^2\)</span> avec 1 degré de liberté.</p>
<p>Dans notre exemple, la statistique du rapport de vraisemblance est
égale à 3.29.</p>
<pre class="r"><code>rv &lt;- -2*log(l_0 / l_max)
rv</code></pre>
<pre><code>## [1] 3.291315</code></pre>
<p>La probabilité d’obtenir un rapport plus grand ou égal à celui-ci, si
l’hypothèse nulle <span class="math inline">\(p = 0.5\)</span> est
vraie, peut être approximée avec la distribution cumulative du <span
class="math inline">\(\chi^2\)</span>.</p>
<pre class="r"><code>1 - pchisq(rv, df = 1)</code></pre>
<pre><code>## [1] 0.06964722</code></pre>
<p><em>Note</em>: Le test du rapport de vraisemblance ne s’applique pas
si l’hypothèse nulle se trouve à la limite des valeurs possibles pour un
paramètre. Par exemple, pour le paramètre <span
class="math inline">\(p\)</span> d’une distribution binomiale, nous ne
pouvons pas utiliser ce test pour l’hypothèse nulle <span
class="math inline">\(p_0 = 0\)</span> ou <span
class="math inline">\(p_0 = 1\)</span>.</p>
</div>
<div id="comparaison-de-modèles" class="section level2">
<h2>Comparaison de modèles</h2>
<p>Le test du rapport de vraisemblance est aussi utilisé pour comparer
deux modèles. Dans ce cas, il faut que les modèles soient nichés,
c’est-à-dire que le modèle plus simple contienne un sous-ensemble des
paramètres du modèle plus complexe. Par exemple, supposons un modèle de
régression linéaire avec 1 prédicteur et un deuxième avec 3
prédicteurs.</p>
<ul>
<li>M1: <span class="math inline">\(y = \beta_0 + \beta_1 x_1 +
\epsilon\)</span></li>
<li>M2: <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2
x_2 + \beta_3 x_3 + \epsilon\)</span></li>
</ul>
<p>Dans ce cas, M1 peut être vu comme une version de M2 où <span
class="math inline">\(\beta_2\)</span> et <span
class="math inline">\(\beta_3\)</span> sont fixés à 0. Si M1 est le vrai
modèle pour les données, la statistique du rapport de vraisemblance
entre les deux modèles suit approximativement une distribution du <span
class="math inline">\(\chi^2\)</span>, avec un nombre de degrés de
liberté égal à la différence du nombre de paramètres estimés entre les
deux modèles (ici, 2).</p>
<p><span class="math display">\[- 2 \left( l_{M1} - l_{M2} \right) \sim
\chi^2(2)\]</span></p>
<p>Dans le cours ECL7102, nous avons étudié la comparaison de modèles
avec le critère d’information d’Akaike (AIC):</p>
<p><span class="math display">\[AIC = - 2 \log L + 2K =  -2l +
2K\]</span></p>
<p>Dans cette formule, <span class="math inline">\(K\)</span> est le
nombre de paramètres ajustables du modèle. Nous avons aussi vu une
correction à l’AIC (AICc) pour les “petits” échantillons (lorsque <span
class="math inline">\(N/K\)</span> &lt; 30, où <span
class="math inline">\(N\)</span> est la taille de l’échantillon).</p>
<p>L’AIC a une portée plus large que le test du rapport de
vraisemblance, car on peut comparer plus de deux modèles, qu’ils soient
nichés ou non. Lorsque les deux méthodes s’appliquent, leurs objectifs
sont différents:</p>
<ul>
<li><p>l’AIC vise à identifier le modèle qui prédirait le mieux la
réponse pour un nouvel échantillon de la même population;</p></li>
<li><p>le test du rapport de vraisemblance indique si l’écart observé
entre l’ajustement du modèle le plus simple et le modèle le plus
complexe est compatible avec l’hypothèse que le modèle le plus simple
soit correct.</p></li>
</ul>
</div>
</div>
<div id="calcul-des-intervalles-de-confiance" class="section level1">
<h1>Calcul des intervalles de confiance</h1>
<p>Si <span class="math inline">\(\hat{\theta}\)</span> est l’estimé du
maximum de vraisemblance pour un paramètre <span
class="math inline">\(\theta\)</span>, nous pouvons obtenir un
intervalle de confiance pour ce paramètre en utilisant la relation entre
test d’hypothèse et intervalle de confiance:</p>
<blockquote>
<p>Si on ne peut pas rejeter l’hypothèse nulle <span
class="math inline">\(\theta = \theta_0\)</span> avec un seuil de
signification <span class="math inline">\(\alpha\)</span>, alors <span
class="math inline">\(\theta_0\)</span> fait partie de l’intervalle de
confiance à <span class="math inline">\(100(1-\alpha)\%\)</span> pour
<span class="math inline">\(\theta\)</span>.</p>
</blockquote>
<p>Par exemple, les limites de l’intervalle de confiance à 95% sont les
valeurs de <span class="math inline">\(\theta\)</span> où la statistique
du rapport de vraisemblance est égale au 95e centile de la distribution
du <span class="math inline">\(\chi^2\)</span>; il s’agit de la valeur
maximale de la statistique qui n’est pas rejetée à un seuil <span
class="math inline">\(\alpha = 0.05\)</span>.</p>
<p><span class="math display">\[- 2 \left( l(\theta_0) - l(\hat{\theta})
\right) = \chi^2_{0.95}(1)\]</span></p>
<p><em>Rappel</em>: Le test du <span
class="math inline">\(\chi^2\)</span> est unilatéral, car seules les
valeurs élevées de la statistique indiquent un écart significatif avec
l’hypothèse nulle.</p>
<p>En isolant <span class="math inline">\(\theta_0\)</span> dans
l’équation, on obtient:</p>
<p><span class="math display">\[l(\theta_0) = l(\hat{\theta}) -
\frac{\chi^2_{0.95}(1)}{2}\]</span></p>
<p>Il s’agit donc de déterminer les valeurs de <span
class="math inline">\(\theta\)</span> pour lequelles la
log-vraisemblance est environ 1.92 inférieure au maximum.</p>
<pre class="r"><code>qchisq(0.95, df = 1) / 2</code></pre>
<pre><code>## [1] 1.920729</code></pre>
<div id="exemple" class="section level2">
<h2>Exemple</h2>
<p>Pour notre exemple de germination de semences (<span
class="math inline">\(\hat{p} = 0.3\)</span>), les limites de
l’intervalle à 95% correspondent à <span class="math inline">\(L =
0.0281\)</span>.</p>
<pre class="r"><code>exp(dbinom(6, 20, 0.3, log = TRUE) - qchisq(0.95, df = 1)/2)</code></pre>
<pre><code>## [1] 0.02807512</code></pre>
<p>Ce seuil est représenté par la ligne pointillée sur le graphique
ci-dessous et correspond à un intervalle approximatif de (0.132, 0.516)
pour <span class="math inline">\(p\)</span>.</p>
<pre class="r"><code>ggplot(NULL) +
    labs(x = &quot;p&quot;, y = &quot;L(p)&quot;) +
    stat_function(geom = &quot;area&quot;, fill = &quot;#d3492a&quot;, n = 1000,
        fun = function(x) ifelse(x &gt; 0.132 &amp; x &lt; 0.516,
                                 dbinom(6, 20, prob = x), NA)) +
    stat_function(fun = function(x) dbinom(6, 20, prob = x), 
                  geom = &quot;density&quot;) +
    geom_hline(yintercept = 0.0279, linetype = &quot;dashed&quot;) +
    scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
    scale_y_continuous(limits = c(0, 0.2), expand = c(0, 0))</code></pre>
<p><img src="03-Maximum_vraisemblance_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Pour une expérience avec le même estimé de <span
class="math inline">\(\hat{p}\)</span>, mais un plus grand échantillon
<span class="math inline">\((n = 50, y = 15)\)</span>, la limite de
<span class="math inline">\(L\)</span> pour l’intervalle à 95% est de
0.0179.</p>
<pre class="r"><code>exp(dbinom(15, 50, 0.3, log = TRUE) - qchisq(0.95, df = 1)/2)</code></pre>
<pre><code>## [1] 0.01792382</code></pre>
<p>Comme on le voit ci-dessous, la fonction de vraisemblance et donc
l’intervalle de confiance sont plus étroits.</p>
<pre class="r"><code>ggplot(NULL) +
    labs(x = &quot;p&quot;, y = &quot;L(p)&quot;) +
    stat_function(geom = &quot;area&quot;, fill = &quot;#d3492a&quot;, n = 1000,
        fun = function(x) ifelse(x &gt; 0.185 &amp; x &lt; 0.435,
                                 dbinom(15, 50, prob = x), NA)) +
    stat_function(fun = function(x) dbinom(15, 50, prob = x), 
                  geom = &quot;density&quot;) +
    geom_hline(yintercept = 0.0179, linetype = &quot;dashed&quot;) +
    scale_x_continuous(limits = c(0, 1), expand = c(0, 0)) +
    scale_y_continuous(breaks = seq(0, 0.12, 0.03), 
                       limits = c(0, 0.13), expand = c(0, 0))</code></pre>
<p><img src="03-Maximum_vraisemblance_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="vraisemblance-profilée" class="section level2">
<h2>Vraisemblance profilée</h2>
<p>Si <span class="math inline">\(m\)</span> paramètres sont estimés en
même temps, la fonction de vraisemblance n’est pas une courbe, mais
plutôt une surface en <span class="math inline">\(m\)</span> dimensions.
Lorsqu’on calcule le rapport de vraisemblance <span
class="math inline">\(- 2 \left( l(\theta_{0}) - l(\hat{\theta})
\right)\)</span> pour différentes valeurs <span
class="math inline">\(\theta_{0}\)</span> d’un des paramètres, il faut
donc choisir quelle valeur donner aux autres <span
class="math inline">\(m - 1\)</span> paramètres. Une solution simple
serait de fixer tous les autres paramètres à leur valeur estimée au
maximum de vraisemblance, mais cela suppose que ces estimés sont
indépendants. En général, si on fixe <span
class="math inline">\(\theta_0\)</span> à une valeur autre que <span
class="math inline">\(\hat{\theta}\)</span>, l’estimé maximisant la
vraisemblance peut changer.</p>
<p>Par exemple, dans le modèle de régression linéaire illustré
ci-dessous, le meilleur estimé de la pente change si on fixe l’ordonnée
à l’origine à 0 (ligne pointillée).</p>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="03-Maximum_vraisemblance_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Afin de construire la courbe de <span
class="math inline">\(l(\theta_0)\)</span> pour différentes valeurs du
paramètre, il faut donc pour chaque valeur fixe de <span
class="math inline">\(\theta_0\)</span> trouver la maximum de
vraisemblance pour le reste des paramètres. La courbe résultante se
nomme la vraisemblance profilée (<em>profile likelihood</em>).</p>
<p>La fonction <code>profile</code> du package <em>bbmle</em> évalue la
vraisemblance profilée de chaque paramètre à partir du résultat de
<code>mle2</code>. Voici le résultat obtenu pour le modèle ajusté plus
tôt (régression binomiale négative du nombre d’espèces de plantes des
îles Galapagos).</p>
<pre class="r"><code>galap_pro &lt;- profile(mle_galap)
plot(galap_pro)</code></pre>
<p><img src="03-Maximum_vraisemblance_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Pour chaque paramètre, le graphique montre la racine carrée du
rapport de vraisemblance <span class="math inline">\(\sqrt{- 2 \left(
l(\theta_{0}) - l(\hat{\theta}) \right)}\)</span> pour la vraisemblance
profilée. La transformation racine carrée permet de voir rapidement si
la log-vraisemblance profilée est approximativement quadratique (voir
section suivante), ce qui résulterait en un “V” symétrique après
transformation.</p>
<p>Différents intervalles de confiance sont superposés au graphique; on
peut aussi obtenir directement ces intervalles avec la fonction
<code>confint</code>.</p>
<pre class="r"><code>confint(galap_pro, level = 0.95)</code></pre>
<pre><code>##             2.5 %     97.5 %
## b_0     3.0259619 3.66809720
## b_area  0.2837173 0.42822254
## b_near -0.2600032 0.05105544
## theta   1.5113578 4.69693757</code></pre>
</div>
<div id="approximation-quadratique" class="section level2">
<h2>Approximation quadratique</h2>
<p>Puisque le calcul de la vraisemblance profilée d’un paramètre
requiert un ajustement répété des autres paramètres du modèle, cette
méthode prend beaucoup de temps pour un modèle complexe.</p>
<p>Une méthode plus approximative, mais beaucoup plus rapide, est de
supposer que la log-vraisemblance suit une forme quadratique. Avec un
seul paramètre, cette forme quadratique est une parabole centrée sur le
maximum de vraisemblance: <span class="math inline">\(- 2 \left(
l(\theta_{0}) - l(\hat{\theta}) \right) = a (\theta_{0} -
\hat{\theta})^2\)</span>. Ici, le coefficient <span
class="math inline">\(a\)</span> mesure la courbure de la parabole.
Comme nous avons vu dans l’exemple binomial ci-dessus, plus cette
courbure est prononcée, plus l’estimation du paramètre est précise.</p>
<p>En fait, si l’approximation quadratique est bonne, la variance de
<span class="math inline">\(\hat{\theta}\)</span> (donc le carré de son
erreur-type) est l’inverse de la dérivée seconde de <span
class="math inline">\(-l\)</span>, qui mesure la courbure au
maximum.</p>
<p><span
class="math display">\[\frac{\textrm{d}^2(-l)}{\textrm{d}\theta^2} =
\frac{1}{\sigma_{\hat{\theta}}^2}\]</span></p>
<p>Avec <span class="math inline">\(m\)</span> paramètres, la courbure
en <span class="math inline">\(m\)</span> dimensions autour du maximum
est représentée par une matrice <span class="math inline">\(m \times
m\)</span> des dérivées partielles secondes de <span
class="math inline">\(-l\)</span>, qu’on appelle la matrice
d’information de Fisher. En inversant cette matrice, on obtient les
variances et covariances des estimés. En supposant que l’approximation
quadratique est juste, ces variances et covariances sont suffisantes
pour obtenir les intervalles de confiance voulus de chaque
paramètre.</p>
<p>Dans le package <em>bbmle</em>, on peut calculer les intervalles de
confiance selon l’approximation quadratique en spécifiant
<code>method = "quad"</code> dans la fonction <code>confint</code>:</p>
<pre class="r"><code>confint(mle_galap, level = 0.95, method = &quot;quad&quot;)</code></pre>
<pre><code>##             2.5 %    97.5 %
## b_0     3.0246480 3.6457823
## b_area  0.2847479 0.4241100
## b_near -0.2536734 0.0451341
## theta   1.1781122 4.2508322</code></pre>
<p>On remarque ici que les estimés s’approchent de ceux de la
vraisemblance profilée, sauf pour <span
class="math inline">\(\theta\)</span>. En inspectant les profils obtenus
plus haut, il est apparent que celui de <span
class="math inline">\(\theta\)</span> suit moins la forme
quadratique.</p>
</div>
</div>
<div id="résumé" class="section level1">
<h1>Résumé</h1>
<ul>
<li><p>Pour un modèle statistique, la vraisemblance est une fonction qui
associe à chaque valeur des paramètres la probabilité des données
observées, conditionnelle à cette paramétrisation. Selon le principe du
maximum de vraisemblance, le meilleur estimé des paramètres est celui
qui maximise la vraisemblance.</p></li>
<li><p>Afin de déterminer le maximum de vraisemblance pour un modèle
personnalisé dans R, il faut créer une fonction qui calcule la
log-vraisemblance en fonction des paramètres, puis faire appel à un
algorithme d’optimisation pour trouver le maximum.</p></li>
<li><p>Le test du rapport de vraisemblance permet de tester une
hypothèse sur la valeur d’un paramètre estimé au moyen du maximum de
vraisemblance, d’obtenir un intervalle de confiance pour ce paramètre,
ou de comparer deux modèles nichés.</p></li>
<li><p>Pour estimer l’incertitude d’un estimé dans un modèle avec
plusieurs paramètres ajustables, nous pouvons soit calculer la
vraisemblance profilée pour ce paramètre, soit avoir recours à
l’approximation quadratique.</p></li>
</ul>
</div>
<div id="références" class="section level1">
<h1>Références</h1>
<ul>
<li><p>Bolker, B.M. (2008) Ecological models and data in R. Princeton
University Press, Princeton, New Jersey. (Chapitre 6 sur le maximum de
vraisemblance)</p></li>
<li><p>Canham, C.D., LePage, P.T. et Coates, K.D. (2004) A neighborhood
analysis of canopy tree competition: effects of shading versus crowding.
Canadian Journal of Forest Research 34: 778–787.</p></li>
<li><p>Clark, J.S., Silman, M., Kern, R., Macklin, E. et
HilleRisLambers, J. (1999) Seed dispersal near and far: Patterns across
temperate and tropical forests. Ecology 80: 1475–1494.</p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
